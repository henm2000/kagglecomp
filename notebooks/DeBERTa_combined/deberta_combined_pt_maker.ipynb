{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Cell 2: Install + Imports --------\n",
    "\n",
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "\n",
    "# HuggingFace imports\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import transformers\n",
    "\n",
    "# Silence annoying warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Cell 3: Load Spelling Corrections + Normalize Function --------\n",
    "\n",
    "# Load spelling corrections\n",
    "spelling_path = \"/kaggle/input/more-data-map/spelling_corrections_v1.csv\"\n",
    "spelling_df = pd.read_csv(spelling_path)\n",
    "spelling_dict = dict(zip(spelling_df['misspelled'], spelling_df['correct']))\n",
    "\n",
    "print(f\"Loaded {len(spelling_dict)} spelling corrections\")\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text using spelling corrections.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    words = text.split()\n",
    "    corrected_words = [spelling_dict.get(word, word) for word in words]\n",
    "    return \" \".join(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Cell 4: Load Data + Normalize + Build Text Column --------\n",
    "\n",
    "# Load train_73k_normalized.csv from Kaggle input\n",
    "train_path = \"/kaggle/input/more-data-map/train_73k_normalized.csv\"\n",
    "train = pd.read_csv(train_path, low_memory=False)\n",
    "\n",
    "print(f\"Loaded {len(train)} samples\")\n",
    "\n",
    "# Build text column from normalized columns\n",
    "train[\"text\"] = (\n",
    "    train[\"QuestionText_Norm\"].fillna(\"\").astype(str) + \" \" +\n",
    "    train[\"MC_Answer_Norm\"].fillna(\"\").astype(str) + \" \" +\n",
    "    train[\"StudentExplanation_Norm\"].fillna(\"\").astype(str)\n",
    ").str.strip()\n",
    "\n",
    "# Label encoding for target column\n",
    "unique_labels = sorted(train[\"target\"].unique())\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "train[\"label\"] = train[\"target\"].map(label2id)\n",
    "\n",
    "print(f\"Total unique target labels: {len(unique_labels)}\")\n",
    "print(f\"Sample labels: {list(unique_labels)[:5]}\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Cell 5: Tokenizer + Dataset Class + KFold --------\n",
    "\n",
    "# --- 1. Load Tokenizer ---\n",
    "MODEL_NAME = \"microsoft/deberta-v3-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"Tokenizer loaded!\")\n",
    "\n",
    "# --- 2. Dataset Class ---\n",
    "class TargetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=256):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- 3. K-Fold Setup ---\n",
    "NUM_FOLDS = 5\n",
    "kfold = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"KFold ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da9f8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.4' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/×—×Ÿ/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# -------- Cell 6: Model Definition + Device --------\n",
    "\n",
    "# Create a fresh model for each fold\n",
    "def create_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(unique_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Cell 7: Full K-Fold Training Loop --------\n",
    "\n",
    "EPOCHS = 5\n",
    "PATIENCE = 2\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kfold.split(train)):\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"===== FOLD {fold+1}/{NUM_FOLDS} =====\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    # ---- Split dataframe ----\n",
    "    train_df = train.iloc[train_idx].reset_index(drop=True)\n",
    "    valid_df = train.iloc[valid_idx].reset_index(drop=True)\n",
    "\n",
    "    # ---- Create datasets ----\n",
    "    train_ds = TargetDataset(train_df, tokenizer)\n",
    "    valid_ds = TargetDataset(valid_df, tokenizer)\n",
    "\n",
    "    # ---- Dataloaders ----\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "    print(f\"Train batches: {len(train_loader)}, Valid batches: {len(valid_loader)}\")\n",
    "\n",
    "    # ---- Fresh model for this fold ----\n",
    "    model = create_model()\n",
    "    model.to(device)\n",
    "\n",
    "    # ---- Optimizer ----\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    # ---- Best model tracking ----\n",
    "    best_val_loss = float(\"inf\")\n",
    "    bad_epochs = 0\n",
    "\n",
    "    # =====================================\n",
    "    #           TRAINING EPOCHS\n",
    "    # =====================================\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n----- Epoch {epoch+1}/{EPOCHS} -----\")\n",
    "\n",
    "        # ===== TRAINING =====\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            if i % 500 == 0:\n",
    "                print(f\"  Training batch {i}/{len(train_loader)}\")\n",
    "            \n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            total_correct += (preds == batch[\"labels\"]).sum().item()\n",
    "            total_samples += len(batch[\"labels\"])\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        accuracy = total_correct / total_samples * 100\n",
    "        \n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Train Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        # ===== VALIDATION =====\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        valid_correct = 0\n",
    "        valid_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(valid_loader):\n",
    "                if i % 500 == 0:\n",
    "                    print(f\"  Validation batch {i}/{len(valid_loader)}\")\n",
    "                \n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                valid_loss += outputs.loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                valid_correct += (preds == batch[\"labels\"]).sum().item()\n",
    "                valid_samples += len(batch[\"labels\"])\n",
    "\n",
    "        avg_val_loss = valid_loss / len(valid_loader)\n",
    "        val_accuracy = valid_correct / valid_samples * 100\n",
    "        \n",
    "        print(f\"Valid Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Valid Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # ===== CHECK IF BEST MODEL =====\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            print(\"ðŸ”¥ New best model! Saving checkpoint...\")\n",
    "            best_val_loss = avg_val_loss\n",
    "            bad_epochs = 0\n",
    "\n",
    "            # Save model state and mappings\n",
    "            save_dict = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'label2id': label2id,\n",
    "                'id2label': id2label,\n",
    "                'num_labels': len(unique_labels)\n",
    "            }\n",
    "            \n",
    "            save_path = f\"/kaggle/working/deberta_combined_fold{fold+1}.pt\"\n",
    "            torch.save(save_dict, save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            print(f\"No improvement ({bad_epochs}/{PATIENCE} bad epochs)\")\n",
    "\n",
    "        # ===== EARLY STOPPING =====\n",
    "        if bad_epochs >= PATIENCE:\n",
    "            print(\"â›” Early stopping triggered â€” stopping training for this fold.\")\n",
    "            break\n",
    "\n",
    "print(\"\\nðŸ”¥ All folds completed! Models saved in /kaggle/working/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
